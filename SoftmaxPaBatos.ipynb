{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SoftmaxPaBatos",
      "provenance": [],
      "authorship_tag": "ABX9TyOAD9smq7lGC23VZCv9bmLM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yolitie/Deep-Learning/blob/main/SoftmaxPaBatos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "ZQhr8BHE07Dq",
        "outputId": "1a183d8d-b12c-4d10-fd59-db16c0f1ccc8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c300ee9026b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMNIST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./data'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             raise RuntimeError('Dataset not found.' +\n\u001b[0m\u001b[1;32m     91\u001b[0m                                ' You can use download=True to download it')\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Dataset not found. You can use download=True to download it"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "dataset= torchvision.datasets.MNIST(root='./data',transform=torchvision.transforms.ToTensor())\n",
        "\n",
        "\n",
        "#ON IMAGES\n",
        "#CenterCrop, Grayscale, Pad, RandomAffine, RandomCrop, RandomHorizontalFlip, RandomRotation, Resize, Scale\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#SOFTMAX AND CROSS-ENTROPY\n",
        "#Softmax== S(yi)=e^yi/SUMATORY(e^yi)\n",
        "#the output value will be between 0 and 1, so we have probabilities.\n",
        "#Let´s say we have a linear layer that has 3 output values and these values are so called scores/LOGITS\n",
        "#Then the values are squashed to be between 0 and 1\n",
        "#That is our prediction and then we can choose the class with the highest probability\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "def softmax(x):\n",
        "  return np.exp(x)/np.sum(np.exp(x),axis=0)\n",
        "\n",
        "x=np.array([2.0,1.0,0.1])\n",
        "outputs= softmax(x)\n",
        "print('softmax numpy:', outputs)\n",
        "\n",
        "x=torch.tensor([2.0,1.0,0.1])\n",
        "outputs=torch.softmax(x,dim=0)\n",
        "#dim=0 it computes ir along the first axis.\n",
        "print(outputs)\n",
        "\n",
        "#Lot of times the softmax function is combined with the cross-entropy function\n",
        "#So this measures the performance of a classification model whose output is a probability between 0 and 1.\n",
        "#It can be used in multiclass problems, the loss increases as the predicted probability diverges from the actual label,\n",
        "#the better our prediction the lower our loss.\n",
        "\n",
        "def cross_entropy(actual,predicted):\n",
        "  loss= -np.sum(actual*np.log(predicted))\n",
        "  return loss #/ float(predicted.shape[0])\n",
        "\n",
        "# y must be one hot encoded\n",
        "# if class 0: [1 0 0]\n",
        "# if class 1: [0 1 0]\n",
        "# if class 2: [0 0 1]\n",
        "Y=np.array([1,0,0])\n",
        "\n",
        "#y_pred has probabilities\n",
        "Y_pred_good= np.array([0.7,0.2,0.1])\n",
        "Y_pred_bad= np.array([0.1,0.3,0.6])\n",
        "l1= cross_entropy(Y,Y_pred_good)\n",
        "l2= cross_entropy(Y,Y_pred_bad)\n",
        "print(f'Loss1 numpy: {l1:.4f}')\n",
        "print(f'Loss2 numpy: {l2:.4f}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cR2xTNiz2_jP",
        "outputId": "26f07733-fa37-4d38-d17b-bd96ed5b4e57"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "softmax numpy: [0.65900114 0.24243297 0.09856589]\n",
            "tensor([0.6590, 0.2424, 0.0986])\n",
            "Loss1 numpy: 0.3567\n",
            "Loss2 numpy: 2.3026\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#DO THIS BUT IN PYTORCH\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "loss= nn.CrossEntropyLoss()\n",
        "#Here we have to be careful because the crossentropyloss already applies the LogSoftmax and then the negative\n",
        "#log likelihood loss. so we should not implement the softmax layer for ourselves.\n",
        "#Our y must not be One-Hot encoded, we shoul only put the correct class label.\n",
        "\n",
        "Y= torch.tensor([0])\n",
        "# Y has the size number of samples times the number of classes , nsamples x nclasses= 1x3\n",
        "#Y_pred_good It is an array of arrays\n",
        "Y_pred_good= torch.tensor([[2.0,1.0,0.1]])\n",
        "Y_pred_bad= torch.tensor([[0.5,2.0,0.3]])\n",
        "\n",
        "l1=loss(Y_pred_good,Y)\n",
        "l2=loss(Y_pred_bad,Y)\n",
        "\n",
        "print(l1.item())\n",
        "print(l2.item())\n",
        "#It only has one value so we can use the item() function\n",
        "\n",
        "#How to get the predictions??\n",
        "# _ because we don´t need this\n",
        "_, predictions1= torch.max(Y_pred_good,1)\n",
        "_, predictions2= torch.max(Y_pred_bad,1)\n",
        "print(predictions1)\n",
        "print(predictions2)\n",
        "\n",
        "#loss in pytorch allows for multiple samples\n",
        "#let´s increase the number of samples\n",
        "\n",
        "#3 samples, so our actual Y has three class labels\n",
        "Y= torch.tensor([2,0,1])\n",
        "\n",
        "#Our predictions must be of size nsamples x nclasses= 3x3\n",
        "Y_pred_good= torch.tensor([[0.1,1.0,2.1],[2.0,1.0,0.1],[0.1,3.0,0.1]])\n",
        "Y_pred_bad= torch.tensor([[2.1,1.0,0.1],[0.1,1.0,2.1],[0.1,3.0,0.1]])\n",
        "\n",
        "l1=loss(Y_pred_good,Y)\n",
        "l2=loss(Y_pred_bad,Y)\n",
        "\n",
        "print(l1.item())\n",
        "print(l2.item())\n",
        "\n",
        "_, predictions1=torch.max(Y_pred_good,1)\n",
        "_, predictions2= torch.max(Y_pred_bad,1)\n",
        "print(predictions1)\n",
        "print(predictions2)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Na2NZuRMLF91",
        "outputId": "fc509d4c-175d-49c2-c9b9-6cc4f28c0ca8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4170299470424652\n",
            "1.840616226196289\n",
            "tensor([0])\n",
            "tensor([1])\n",
            "0.3018244206905365\n",
            "1.6241613626480103\n",
            "tensor([2, 0, 1])\n",
            "tensor([0, 2, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#HOW A TYPICAL NN LOOKS LIKE\n",
        "#Multiclass problem\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class NeuralNet2(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_classes):\n",
        "    super(NeuralNet2,self).__init__()\n",
        "    self.linear1=nn.Linear(input_size, hidden_size)\n",
        "    self.relu=nn.ReLU()\n",
        "    self.linear2= nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "  def forward(self,x):\n",
        "    out=self.linear1(x)\n",
        "    out=self.relu(out)\n",
        "    out=self.linear2(out)\n",
        "    #no softmax at the end\n",
        "    return out\n",
        "\n",
        "model= NeuralNet2(input_size=28*28, hidden_size=5, num_classes=3)\n",
        "criterion= nn.CrossEntropyLoss() #(applies Softmax)\n"
      ],
      "metadata": {
        "id": "xyq-HeiIQI6W"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#BINARY CLASS \n",
        "import torch \n",
        "import torch.nn as nn\n",
        "\n",
        "#Binary classification\n",
        "class NeuralNet1(nn.Module):\n",
        "  def __init__(self, input_size,hidden_size):\n",
        "    super(NeuralNet1,self).__init__()\n",
        "    self.linear1=nn.Linear(input_size,hidden_size)\n",
        "    self.relu=nn.ReLU()\n",
        "    self.linear2=nn.Linear(hidden_size,1)\n",
        "  \n",
        "  def forward(self,x):\n",
        "    out=self.linear1(x)\n",
        "    out=self.relu(out)\n",
        "    out=self.linear2(out)\n",
        "    #sigmoid at the end\n",
        "    y_pred= torch.sigmoid(out)\n",
        "    return y_pred\n",
        "\n",
        "model= NeuralNet1(input_size=28*28, hidden_size=5)\n",
        "criterion= nn.BCELoss()"
      ],
      "metadata": {
        "id": "CiqKeV25R02A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}