{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TransformerPaBatos",
      "provenance": [],
      "authorship_tag": "ABX9TyOw1OsPl/Kkn1wLURy07rlk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yolitie/Deep-Learning/blob/main/TransformerPaBatos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dbOCQqCP0-Ss"
      },
      "outputs": [],
      "source": [
        "#ATTENTION IS ALL YOU NEED\n",
        "#We have an encoder on the left and on the right a decoder.\n",
        "#Starting from the bottom we have some input, letÂ´s say some source texte\n",
        "#We are going to create some embedders. The input is going to be sent to a multi-head attention.\n",
        "#Then is going to go to a normalization, then to a feed forward and then again to a normalization.\n",
        "#Decoder Block, transformer block and has an additional masked multi-head and normalization.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self, embed_size, heads): #In how many parts we split is called heads.\n",
        "    super(SelfAttention, self).__init__()\n",
        "    self.embed_size=embed_size\n",
        "    self.heads=heads\n",
        "    self.head_dim=embed_size//heads #integer division, sometimes the integer division is not possible so we put an assert\n",
        "\n",
        "    assert (self.head_dim*heads==embed_size), \"Embed size needs to be divided by heads\"\n",
        "\n",
        "    self.values=nn.Linear(self.head_dim,self.head_dim, bias=False)\n",
        "    self.keys= nn.Linear(self.head_dim, self.head_dim, bias= False)\n",
        "    self.queries=nn.Linear(self.head_dim, self.head_dim, bias= False)\n",
        "    self.fc_out=nn.Linear(heads*self.head_dim, embed_size)\n",
        "\n",
        "  def forward(self,values,keys,query,mask):\n",
        "    N=query.shape[0] #How many examples are we sending at the same time\n",
        "    value_len, key_len, query_len= values.shape[1],keys.shape[1],query.shape[1]\n",
        "\n",
        "    #Split embedding into self.heads pieces\n",
        "    values=values.reshape(N, value_len,self.heads,self.head_dim)\n",
        "    keys=keys.reshape(N,key_len,self.heads,self.head_dim)\n",
        "    queries=query.reshape(N,query_len,self.heads,self.head_dim)\n",
        "\n",
        "    values= self.values(values)\n",
        "    keys= self.keys(keys)\n",
        "    queries=self.queries(queries)\n",
        "    \n",
        "    energy= torch.einsum(\"nqhd,nkhd--> nhqk\",[queries,keys])\n",
        "    #queries shape: (N,query_len,heads,heads_dim)\n",
        "    #keys shape: (N, key_len, heads,heads_dim)\n",
        "    #energy shape: (N,heads, query_len, key_len) query_len is the target source sentence, and the \n",
        "#key_len is the is the source sentence. so for each word in our target how much should we pay attention to each word in our input in the source sentence.\n",
        "\n",
        "    if mask is not None:\n",
        "      energy=energy.masked_fill(mask==0,float(\"-1e20\"))\n",
        "    \n",
        "    attention=torch.softmax(energy/(self.embed_size**(1/2)),dim=3)\n",
        "    \n",
        "    out= torch.einsum(\"nhql,nlhd-->nqhd\",[attention,values]).reshape(N,query_len,self.heads*self.head_dim)\n",
        "    #attention shape: (N,heads,query_len,key_len)\n",
        "    #values shape: (N, value_len,heads, heads_dim)\n",
        "    # after einsum(N, query_len, heads, head_dim)then flatten last two dimensions\n",
        "    out=self.fc_out(out)\n",
        "    return out\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self,embed_size, heads, dropout, forward_expansion):\n",
        "    super(TransformerBlock,self).__init__()\n",
        "    self.attention=SelfAttention(embed_size,heads)\n",
        "    self.norm1=nn.LayerNorm(embed_size)\n",
        "    self.norm2= nn.LayerNorm(embed_size)\n",
        "\n",
        "    self.feed_forward=nn.Sequential(nn.Linear(embed_size,forward_expansion*embed_size),nn.ReLU(),nn.Linear(forward_expansion*embed_size,embed_size))\n",
        "    self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self,value,key,query,mask):\n",
        "    attention=self.attention(value,key,query,mask)\n",
        "\n",
        "    x=self.dropout(self,norm1(attention+query))\n",
        "    forward= self.feed_forward(x)\n",
        "    out=self.dropout(self.norm2(forward+x))\n",
        "    return out\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self,src_vocab_size,embed_size,num_layers,heads,device,forward_expansion,dropout,max_length):\n",
        "    super(Encoder,self).__init__()\n",
        "    self.embed_size=embed_size\n",
        "    self.device=device\n",
        "    self.word_embedding=nn.Embedding(src_vocab_size,embed_size)\n",
        "    self.position_embedding=nn.Embedding(max_length,embed_size)\n",
        "\n",
        "    self.layers=nn.ModuleList([TransformerBlock(embed_size,heads,dropout=dropout,forward_expansion=forward_expansion) for _ in range (num_layers)])\n",
        "    self.dropout= nn.Dropout(dropout)\n",
        "  \n",
        "  def forward(self,x,mask):\n",
        "    N,seq_length=x.shape\n",
        "    positions=torch.arange(0,seq_length).expand(N,seq_length).to(self.device)\n",
        "\n",
        "    out=self.dropout(self.word_embedding(x)+self.position_embedding(positions))\n",
        "\n",
        "    for layer in self.layers:\n",
        "      out=layer(out,out,out,mask)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "  def __init__(self, embed_size, heads, forward_expansion, dropout, device):\n",
        "    super(DecoderBlock,self).__init__()\n",
        "    self.attention=SelfAttention(embed_size,heads)\n",
        "    self.norm=nn.LayerNorm(embed_size)\n",
        "    self.transformer_block=TransformerBlock(embed_size,heads,dropout,forward_expansion)\n",
        "    self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self,x,value,key,src_mask,trg_mask):\n",
        "    attention= self.attention(x,x,x,trg_mask)\n",
        "    query= self.dropout(self.norm(attention+x))\n",
        "    our= self.transformer_block(value,key,query,src_mask)\n",
        "    return out\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self,trg_vocab_size,embed_size,num_layers,heads,forward_expansion,dropout,device,max_length):\n",
        "    super(Decoder,self).__init__()\n",
        "    self.device=device\n",
        "    self.word_embedding=nn.Embedding(trg_vocab_size,embed_size)\n",
        "    self.position_embedding=nn.Embedding(max_length,embed_size)\n",
        "\n",
        "    self.layers=nn.ModuleList([DecoderBlock(embed_size,heads,forward_expansion,dropout,device)for _ in range(num_layers)])\n",
        "    self.fc_out=nn.Linear(embed_size,trg_vocab_size)\n",
        "    self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self,x,enc_out,src_mask,trg_mask):\n",
        "    N,seq_length= x.shape\n",
        "    position= torch.arange(0,seq_length).expand(N,seq_length).to(self.device)\n",
        "    x= self.dropout((self.word_embedding(x)+self.position_embedding(positions)))\n",
        "\n",
        "    for layer in self.layers:\n",
        "      x= layer(x,enc_out, enc_out,src_mask,trg_mask)\n",
        "      \n",
        "    out=self.fc_out(x)\n",
        "    return out\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "  def __init__(self,src_vocab_size,trg_vocab_size,src_pad_idx,trg_pad_idx, embed_size=256, num_layers=6, forward_expansion=4,heads=8,dropout=0,device=\"cuda\",max_length=100):\n",
        "    super(Transformer,self).__init__()\n",
        "\n",
        "    self.encoder=Encoder(src_vocab_size,embed_size,num_layers,heads,device,forward_expansion,dropout, max_length)\n",
        "    self.decoder=Decoder(trg_vocab_size,embed_size,num_layers,heads,forward_expansion,dropout,device,max_length)\n",
        "    self.src_pad_idx=src_pad_idx\n",
        "    self.trg_pad_idx=trg_pad_idx\n",
        "    self.device=device\n",
        "\n",
        "  def make_src_mask(self,src):\n",
        "    src_mask=(src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "    #(N,1,1,src_len)\n",
        "    return src_mask.to(self.device)\n",
        "  \n",
        "  def make_trg_mask(self,trg):\n",
        "    N, trg_len=trg.shape\n",
        "    trg_mask=torch.trill(torch.ones((trg_len,trg_len))).expand(N,1,trg_len,trg_len)\n",
        "    return trg_mask.to(self.device)\n",
        "\n",
        "  def forward(self,src,trg):\n",
        "    src_mask=self.make_src_mask(src)\n",
        "    trg_mask=self.make_trg_mask(trg)\n",
        "    enc_src=self.encoder(src,src_mask)\n",
        "    out=self.decoder(trg,enc_src,src_mask,trg_mask)\n",
        "    return out\n",
        "\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "  device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  x=torch.tensor([[1,5,6,4,3,9,5,2,0],[1,8,7,3,4,5,6,7,2]]).to(device)\n",
        "  trg=torch.tensor([[1,7,4,3,5,9,2,0],[1,5,6,2,4,7,6,2]]).to(device)\n",
        "\n",
        "  src_pad_idx=0\n",
        "  trg_pad_idx=0\n",
        "  src_vocab_size=10\n",
        "  trg_vocab_size=10\n",
        "  model= Transformer(src_vocab_size,trg_vocab_size,src_pad_idx,trg_pad_idx).to(device)\n",
        "\n",
        "  out=model(x,trg[:,-1])\n",
        "  print(out.shape)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "KYlzCuz-3Hbz",
        "outputId": "f9a588ba-857e-4948-f05a-e86c7fd160f9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-dc8a248078d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    164\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrg_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msrc_pad_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrg_pad_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m   \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-dc8a248078d2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, trg)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     \u001b[0msrc_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_src_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m     \u001b[0mtrg_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_trg_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0menc_src\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-dc8a248078d2>\u001b[0m in \u001b[0;36mmake_src_mask\u001b[0;34m(self, src)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0msrc_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_pad_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;31m#(N,1,1,src_len)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mmake_trg_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;31m# This function throws if there's a driver initialization error, no GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;31m# are found or any other error occurs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Xi4lcncz-jhK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}