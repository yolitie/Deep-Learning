{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EEG",
      "provenance": [],
      "authorship_tag": "ABX9TyNsjbVdPDRDtB7lwz+leyNj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yolitie/Deep-Learning/blob/main/EEG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "GJYxNZbXZYJf",
        "outputId": "f59bca60-7abe-42e7-f5be-17dac837e169"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-2547d3f8aff2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0meinops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrearrange\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0meinops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRearrange\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReduce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcommon_spatial_pattern\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'einops'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "\"\"\"\n",
        "Transformer for EEG classification\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import os #nos permite acceder a funcionalidades dependientes del Sistema operativo. sobre todo aquellas que nos refieren infrmación sobre el entorno del mismo y nos permite manipular la estructura de directorios.\n",
        "import numpy as np #para calculos cientificos, paquete de procesamiento de matrices de uso general.\n",
        "import math  #paquete que proporciona acceso a las funciones matematicas definidas en el estandar de C.\n",
        "import random  #incluye un conjunto de funciones que permiten obtener de distintos modos numeros aleatorios (pseudoaleatorios).\n",
        "import time    #proporciona un conjunto de funciones para trabajar con fechas y/o horas.\n",
        "import scipy.io  #contiene módulos para optimización, algebra lineal, integracion, interpolacion...\n",
        "\n",
        "from torch.utils.data import DataLoader  #dataset that allows you to use preloaded datasets as well as your own data, dataset stores the samples and their corresponding labels\n",
        "#dataloader wraps an iterable around the dataset to enable easy access to the samples.\n",
        "from torch.autograd import Variable \n",
        "#torch.autograd does automatic differentiation by collecting all gradients.\n",
        "#Autograd is an engine to calculate derivatives, it records a graph pf all the operations performed on a gradient enabled tensor and creates a graph of all the operations performed on a gradient enabled tensor and created an acyclic graph called the dynamic computational graph.\n",
        "#torch.autograd.variable class was used to create tensors that support gradient calculations and operation tracking.\n",
        "#But now torch.Tensor and torch.autograd.Variable are now the same class.\n",
        "from torchsummary import summary\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch import nn\n",
        "from torch import Tensor\n",
        "\n",
        "from einops import rearrange, reduce, repeat\n",
        "from einops.layers.torch import Rearrange, Reduce\n",
        "from common_spatial_pattern import csp\n",
        "# from confusion_matrix import plot_confusion_matrix\n",
        "# from cm_no_normal import plot_confusion_matrix_nn\n",
        "# from torchsummary import summary\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.backends import cudnn\n",
        "cudnn.benchmark = False\n",
        "cudnn.deterministic = True\n",
        "\n",
        "# writer = SummaryWriter('/home/syh/Documents/MI/code/Trans/TensorBoardX/')\n",
        "\n",
        "# torch.cuda.set_device(6)\n",
        "gpus = [6]\n",
        "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = ','.join(map(str, gpus))\n",
        "\n",
        "#init() todas las clases tienen este metodo que se ejecuta cuando la clase esta siendo instanciada ys e crea un objeto.\n",
        "#se utiliza para asignar valores a los atributos de un objeto.\n",
        "#SELF es una referencia a la instancia actual de la clase. usandolo podemos acceder a los atributos y metodos de la clase en python.\n",
        "#metodos del objeto: los objetos tambien contienen metodos, que son funciones que especifican un comportamiento determinado.\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, emb_size):\n",
        "        # self.patch_size = patch_size\n",
        "        super().__init__()\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Conv2d(1, 2, (1, 51), (1, 1)),\n",
        "            nn.BatchNorm2d(2),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(2, emb_size, (16, 5), stride=(1, 5)),\n",
        "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
        "        )\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_size))\n",
        "        # self.positions = nn.Parameter(torch.randn((100 + 1, emb_size)))\n",
        "        # self.positions = nn.Parameter(torch.randn((2200 + 1, emb_size)))\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        b, _, _, _ = x.shape\n",
        "        x = self.projection(x)\n",
        "        cls_tokens = repeat(self.cls_token, '() n e -> b n e', b=b)\n",
        "\n",
        "        # position\n",
        "        # x += self.positions\n",
        "        return x\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, emb_size, num_heads, dropout):\n",
        "        super().__init__()\n",
        "        self.emb_size = emb_size\n",
        "        self.num_heads = num_heads\n",
        "        self.keys = nn.Linear(emb_size, emb_size)\n",
        "        self.queries = nn.Linear(emb_size, emb_size)\n",
        "        self.values = nn.Linear(emb_size, emb_size)\n",
        "        self.att_drop = nn.Dropout(dropout)\n",
        "        self.projection = nn.Linear(emb_size, emb_size)\n",
        "\n",
        "    def forward(self, x: Tensor, mask: Tensor = None) -> Tensor:\n",
        "        queries = rearrange(self.queries(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
        "        keys = rearrange(self.keys(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
        "        values = rearrange(self.values(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
        "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)  # batch, num_heads, query_len, key_len\n",
        "        if mask is not None:\n",
        "            fill_value = torch.finfo(torch.float32).min\n",
        "            energy.mask_fill(~mask, fill_value)\n",
        "\n",
        "        scaling = self.emb_size ** (1 / 2)\n",
        "        att = F.softmax(energy / scaling, dim=-1)\n",
        "        att = self.att_drop(att)\n",
        "        out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
        "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
        "        out = self.projection(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResidualAdd(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        res = x\n",
        "        x = self.fn(x, **kwargs)\n",
        "        x += res\n",
        "        return x\n",
        "\n",
        "\n",
        "class FeedForwardBlock(nn.Sequential):\n",
        "    def __init__(self, emb_size, expansion, drop_p):\n",
        "        super().__init__(\n",
        "            nn.Linear(emb_size, expansion * emb_size),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(drop_p),\n",
        "            nn.Linear(expansion * emb_size, emb_size),\n",
        "        )\n",
        "\n",
        "class GELU(nn.Module):\n",
        "    def forward(self, input: Tensor) -> Tensor:\n",
        "        return input*0.5*(1.0+torch.erf(input/math.sqrt(2.0)))\n",
        "\n",
        "\n",
        "class TransformerEncoderBlock(nn.Sequential):\n",
        "    def __init__(self,\n",
        "                 emb_size,\n",
        "                 num_heads=5,\n",
        "                 drop_p=0.5,\n",
        "                 forward_expansion=4,\n",
        "                 forward_drop_p=0.5):\n",
        "        super().__init__(\n",
        "            ResidualAdd(nn.Sequential(\n",
        "                nn.LayerNorm(emb_size),\n",
        "                MultiHeadAttention(emb_size, num_heads, drop_p),\n",
        "                nn.Dropout(drop_p)\n",
        "            )),\n",
        "            ResidualAdd(nn.Sequential(\n",
        "                nn.LayerNorm(emb_size),\n",
        "                FeedForwardBlock(\n",
        "                    emb_size, expansion=forward_expansion, drop_p=forward_drop_p),\n",
        "                nn.Dropout(drop_p)\n",
        "            )\n",
        "            ))\n",
        "\n",
        "\n",
        "class TransformerEncoder(nn.Sequential):\n",
        "    def __init__(self, depth, emb_size):\n",
        "        super().__init__(*[TransformerEncoderBlock(emb_size) for _ in range(depth)])\n",
        "\n",
        "\n",
        "class ClassificationHead(nn.Sequential):\n",
        "    def __init__(self, emb_size, n_classes):\n",
        "        super().__init__()\n",
        "        self.clshead = nn.Sequential(\n",
        "            Reduce('b n e -> b e', reduction='mean'),\n",
        "            nn.LayerNorm(emb_size),\n",
        "            nn.Linear(emb_size, n_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.clshead(x)\n",
        "        return x, out\n",
        "\n",
        "\n",
        "class ViT(nn.Sequential):\n",
        "    def __init__(self, emb_size=10, depth=3, n_classes=4, **kwargs):\n",
        "        super().__init__(\n",
        "            # channel_attention(),\n",
        "            ResidualAdd(\n",
        "                nn.Sequential(\n",
        "                    nn.LayerNorm(1000),\n",
        "                    channel_attention(),\n",
        "                    nn.Dropout(0.5),\n",
        "                )\n",
        "            ),\n",
        "\n",
        "            PatchEmbedding(emb_size),\n",
        "            TransformerEncoder(depth, emb_size),\n",
        "            ClassificationHead(emb_size, n_classes)\n",
        "        )\n",
        "\n",
        "\n",
        "class channel_attention(nn.Module):\n",
        "    def __init__(self, sequence_num=1000, inter=30):\n",
        "        super(channel_attention, self).__init__()\n",
        "        self.sequence_num = sequence_num\n",
        "        self.inter = inter\n",
        "        self.extract_sequence = int(self.sequence_num / self.inter)  # You could choose to do that for less computation\n",
        "\n",
        "        self.query = nn.Sequential(\n",
        "            nn.Linear(16, 16),\n",
        "            nn.LayerNorm(16),  # also may introduce improvement to a certain extent\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "        self.key = nn.Sequential(\n",
        "            nn.Linear(16, 16),\n",
        "            # nn.LeakyReLU(),\n",
        "            nn.LayerNorm(16),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "        # self.value = self.key\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Linear(16, 16),\n",
        "            # nn.LeakyReLU(),\n",
        "            nn.LayerNorm(16),\n",
        "            nn.Dropout(0.3),\n",
        "        )\n",
        "\n",
        "        self.drop_out = nn.Dropout(0)\n",
        "        self.pooling = nn.AvgPool2d(kernel_size=(1, self.inter), stride=(1, self.inter))\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_normal_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        temp = rearrange(x, 'b o c s->b o s c')\n",
        "        temp_query = rearrange(self.query(temp), 'b o s c -> b o c s')\n",
        "        temp_key = rearrange(self.key(temp), 'b o s c -> b o c s')\n",
        "\n",
        "        channel_query = self.pooling(temp_query)\n",
        "        channel_key = self.pooling(temp_key)\n",
        "\n",
        "        scaling = self.extract_sequence ** (1 / 2)\n",
        "\n",
        "        channel_atten = torch.einsum('b o c s, b o m s -> b o c m', channel_query, channel_key) / scaling\n",
        "\n",
        "        channel_atten_score = F.softmax(channel_atten, dim=-1)\n",
        "        channel_atten_score = self.drop_out(channel_atten_score)\n",
        "\n",
        "        out = torch.einsum('b o c s, b o c m -> b o c s', x, channel_atten_score)\n",
        "        '''\n",
        "        projections after or before multiplying with attention score are almost the same.\n",
        "        '''\n",
        "        out = rearrange(out, 'b o c s -> b o s c')\n",
        "        out = self.projection(out)\n",
        "        out = rearrange(out, 'b o s c -> b o c s')\n",
        "        return out\n",
        "\n",
        "\n",
        "class Trans():\n",
        "    def __init__(self, nsub):\n",
        "        super(Trans, self).__init__()\n",
        "        self.batch_size = 50\n",
        "        self.n_epochs = 2000\n",
        "        self.img_height = 22\n",
        "        self.img_width = 600\n",
        "        self.channels = 1\n",
        "        self.c_dim = 4\n",
        "        self.lr = 0.0002\n",
        "        self.b1 = 0.5\n",
        "        self.b2 = 0.9\n",
        "        self.nSub = nsub\n",
        "        self.start_epoch = 0\n",
        "        self.root = '...'  # the path of data\n",
        "\n",
        "        self.pretrain = False\n",
        "\n",
        "        self.log_write = open(\"results/log_subject%d.txt\" % self.nSub, \"w\")\n",
        "\n",
        "        self.img_shape = (self.channels, self.img_height, self.img_width)  # something no use\n",
        "\n",
        "        self.Tensor = torch.cuda.FloatTensor\n",
        "        self.LongTensor = torch.cuda.LongTensor\n",
        "\n",
        "        self.criterion_l1 = torch.nn.L1Loss().cuda()\n",
        "        self.criterion_l2 = torch.nn.MSELoss().cuda()\n",
        "        self.criterion_cls = torch.nn.CrossEntropyLoss().cuda()\n",
        "\n",
        "        self.model = ViT().cuda()\n",
        "        self.model = nn.DataParallel(self.model, device_ids=[i for i in range(len(gpus))])\n",
        "        self.model = self.model.cuda()\n",
        "        summary(self.model, (1, 16, 1000))\n",
        "\n",
        "        self.centers = {}\n",
        "\n",
        "    def get_source_data(self):\n",
        "\n",
        "        # to get the data of target subject\n",
        "        self.total_data = scipy.io.loadmat(self.root + 'A0%dT.mat' % self.nSub)\n",
        "        self.train_data = self.total_data['data']\n",
        "        self.train_label = self.total_data['label']\n",
        "\n",
        "        self.train_data = np.transpose(self.train_data, (2, 1, 0))\n",
        "        self.train_data = np.expand_dims(self.train_data, axis=1)\n",
        "        self.train_label = np.transpose(self.train_label)\n",
        "\n",
        "        self.allData = self.train_data\n",
        "        self.allLabel = self.train_label[0]\n",
        "\n",
        "        # test data\n",
        "        # to get the data of target subject\n",
        "        self.test_tmp = scipy.io.loadmat(self.root + 'A0%dE.mat' % self.nSub)\n",
        "        self.test_data = self.test_tmp['data']\n",
        "        self.test_label = self.test_tmp['label']\n",
        "\n",
        "        # self.train_data = self.train_data[250:1000, :, :]\n",
        "        self.test_data = np.transpose(self.test_data, (2, 1, 0))\n",
        "        self.test_data = np.expand_dims(self.test_data, axis=1)\n",
        "        self.test_label = np.transpose(self.test_label)\n",
        "\n",
        "        self.testData = self.test_data\n",
        "        self.testLabel = self.test_label[0]\n",
        "\n",
        "        # Mix the train and test data - a quick way to get start\n",
        "        # But I agree, just shuffle data is a bad measure\n",
        "        # You could choose cross validation, or get more data from more subjects, then Leave one subject out\n",
        "        all_data = np.concatenate((self.allData, self.testData), 0)\n",
        "        all_label = np.concatenate((self.allLabel, self.testLabel), 0)\n",
        "        all_shuff_num = np.random.permutation(len(all_data))\n",
        "        all_data = all_data[all_shuff_num]\n",
        "        all_label = all_label[all_shuff_num]\n",
        "\n",
        "        self.allData = all_data[:516]\n",
        "        self.allLabel = all_label[:516]\n",
        "        self.testData = all_data[516:]\n",
        "        self.testLabel = all_label[516:]\n",
        "\n",
        "        # standardize\n",
        "        target_mean = np.mean(self.allData)\n",
        "        target_std = np.std(self.allData)\n",
        "        self.allData = (self.allData - target_mean) / target_std\n",
        "        self.testData = (self.testData - target_mean) / target_std\n",
        "\n",
        "        tmp_alldata = np.transpose(np.squeeze(self.allData), (0, 2, 1))\n",
        "        Wb = csp(tmp_alldata, self.allLabel-1)  # common spatial pattern\n",
        "        self.allData = np.einsum('abcd, ce -> abed', self.allData, Wb)\n",
        "        self.testData = np.einsum('abcd, ce -> abed', self.testData, Wb)\n",
        "        return self.allData, self.allLabel, self.testData, self.testLabel\n",
        "\n",
        "    def update_lr(self, optimizer, lr):\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "\n",
        "    # Do some data augmentation is a potential way to improve the generalization ability\n",
        "    def aug(self, img, label):\n",
        "        aug_data = []\n",
        "        aug_label = []\n",
        "        return aug_data, aug_label\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "\n",
        "        img, label, test_data, test_label = self.get_source_data()\n",
        "        img = torch.from_numpy(img)\n",
        "        label = torch.from_numpy(label - 1)\n",
        "\n",
        "\n",
        "        dataset = torch.utils.data.TensorDataset(img, label)\n",
        "        self.dataloader = torch.utils.data.DataLoader(dataset=dataset, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "        test_data = torch.from_numpy(test_data)\n",
        "        test_label = torch.from_numpy(test_label - 1)\n",
        "        test_dataset = torch.utils.data.TensorDataset(test_data, test_label)\n",
        "        self.test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "        # Optimizers\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr, betas=(self.b1, self.b2))\n",
        "\n",
        "        test_data = Variable(test_data.type(self.Tensor))\n",
        "        test_label = Variable(test_label.type(self.LongTensor))\n",
        "\n",
        "        bestAcc = 0\n",
        "        averAcc = 0\n",
        "        num = 0\n",
        "        Y_true = 0\n",
        "        Y_pred = 0\n",
        "\n",
        "        # Train the cnn model\n",
        "        total_step = len(self.dataloader)\n",
        "        curr_lr = self.lr\n",
        "        # some better optimization strategy is worthy to explore. Sometimes terrible over-fitting.\n",
        "\n",
        "\n",
        "        for e in range(self.n_epochs):\n",
        "            in_epoch = time.time()\n",
        "            self.model.train()\n",
        "            for i, (img, label) in enumerate(self.dataloader):\n",
        "\n",
        "                img = Variable(img.cuda().type(self.Tensor))\n",
        "                label = Variable(label.cuda().type(self.LongTensor))\n",
        "                tok, outputs = self.model(img)\n",
        "                loss = self.criterion_cls(outputs, label)\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "            out_epoch = time.time()\n",
        "\n",
        "            if (e + 1) % 1 == 0:\n",
        "                self.model.eval()\n",
        "                Tok, Cls = self.model(test_data)\n",
        "\n",
        "                loss_test = self.criterion_cls(Cls, test_label)\n",
        "                y_pred = torch.max(Cls, 1)[1]\n",
        "                acc = float((y_pred == test_label).cpu().numpy().astype(int).sum()) / float(test_label.size(0))\n",
        "                train_pred = torch.max(outputs, 1)[1]\n",
        "                train_acc = float((train_pred == label).cpu().numpy().astype(int).sum()) / float(label.size(0))\n",
        "                print('Epoch:', e,\n",
        "                      '  Train loss:', loss.detach().cpu().numpy(),\n",
        "                      '  Test loss:', loss_test.detach().cpu().numpy(),\n",
        "                      '  Train accuracy:', train_acc,\n",
        "                      '  Test accuracy is:', acc)\n",
        "                self.log_write.write(str(e) + \"    \" + str(acc) + \"\\n\")\n",
        "                num = num + 1\n",
        "                averAcc = averAcc + acc\n",
        "                if acc > bestAcc:\n",
        "                    bestAcc = acc\n",
        "                    Y_true = test_label\n",
        "                    Y_pred = y_pred\n",
        "\n",
        "        torch.save(self.model.module.state_dict(), 'model.pth')\n",
        "        averAcc = averAcc / num\n",
        "        print('The average accuracy is:', averAcc)\n",
        "        print('The best accuracy is:', bestAcc)\n",
        "        self.log_write.write('The average accuracy is: ' + str(averAcc) + \"\\n\")\n",
        "        self.log_write.write('The best accuracy is: ' + str(bestAcc) + \"\\n\")\n",
        "\n",
        "        return bestAcc, averAcc, Y_true, Y_pred\n",
        "\n",
        "\n",
        "def main():\n",
        "    best = 0\n",
        "    aver = 0\n",
        "    result_write = open(\"results/sub_result.txt\", \"w\")\n",
        "\n",
        "    for i in range(9):\n",
        "        seed_n = np.random.randint(500)\n",
        "        print('seed is ' + str(seed_n))\n",
        "        random.seed(seed_n)\n",
        "        np.random.seed(seed_n)\n",
        "        torch.manual_seed(seed_n)\n",
        "        torch.cuda.manual_seed(seed_n)\n",
        "        torch.cuda.manual_seed_all(seed_n)\n",
        "        print('Subject %d' % (i+1))\n",
        "        trans = Trans(i + 1)\n",
        "        bestAcc, averAcc, Y_true, Y_pred = trans.train()\n",
        "        print('THE BEST ACCURACY IS ' + str(bestAcc))\n",
        "        result_write.write('Subject ' + str(i + 1) + ' : ' + 'Seed is: ' + str(seed_n) + \"\\n\")\n",
        "        result_write.write('**Subject ' + str(i + 1) + ' : ' + 'The best accuracy is: ' + str(bestAcc) + \"\\n\")\n",
        "        result_write.write('Subject ' + str(i + 1) + ' : ' + 'The average accuracy is: ' + str(averAcc) + \"\\n\")\n",
        "        # plot_confusion_matrix(Y_true, Y_pred, i+1)\n",
        "        best = best + bestAcc\n",
        "        aver = aver + averAcc\n",
        "        if i == 0:\n",
        "            yt = Y_true\n",
        "            yp = Y_pred\n",
        "        else:\n",
        "            yt = torch.cat((yt, Y_true))\n",
        "            yp = torch.cat((yp, Y_pred))\n",
        "\n",
        "\n",
        "    best = best / 9\n",
        "    aver = aver / 9\n",
        "    # plot_confusion_matrix(yt, yp, 666)\n",
        "    result_write.write('**The average Best accuracy is: ' + str(best) + \"\\n\")\n",
        "    result_write.write('The average Aver accuracy is: ' + str(aver) + \"\\n\")\n",
        "    result_write.close()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " la herencia, que permite que una clase use los atributos y métodos de otra. La clase hijo se deriva de la clase padre.\n",
        " La clase hijo sobreescribe o extiende la funcionalidad (métodos y atributos) de la clase padre, es decir, hereda todos los métodos y atributos de la clase padre pero también puede modificarlos o definir los suyos propios\n",
        " En PyTorch, cuando definimos un modelo de deep learning, normalmente definimos una clase que extiende la clase torch.nn.Module, para aprovechar los atributos y métodos que está clase tiene definidos.\n",
        " Para ello, es necesario:\n",
        "\n",
        "    En el constructor del modelo init() inicializamos la superclase con super.init(), de esta forma podemos usar los atributos de la clase padre torch.nn.Module.\n",
        "    En init() también definimos los componentes que queremos que tenga el modelo, como por ejemplo cada una de las capas.\n",
        "    Sobreescribimos el método forward que realiza el forward pass con las operaciones para calcular la/s salida/s.\n",
        "    El método call() llama al método forward por lo que el objeto creado cuando instanciamos la clase definida será directamente «llamable»."
      ],
      "metadata": {
        "id": "vgPYowjVgK7f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "uyWU_2x1gG1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://mlearninglab.com/2020/03/14/curso-de-introduccion-a-pytorch/\n"
      ],
      "metadata": {
        "id": "poI9yblHiE-H"
      }
    }
  ]
}