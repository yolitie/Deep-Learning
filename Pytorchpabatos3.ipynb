{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorchpabatos3",
      "provenance": [],
      "authorship_tag": "ABX9TyNSRc+y6ghg4K9MBRfbKeMS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yolitie/Deep-Learning/blob/main/Pytorchpabatos3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Mvft1Ox5qYU",
        "outputId": "98082fa6-f697-40ce-8287-a8648fbe08ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1., grad_fn=<PowBackward0>)\n",
            "tensor(-2.)\n"
          ]
        }
      ],
      "source": [
        "#Backpropagation\n",
        "#Chain rule, we have two operations or two functions. First, we have an input x and we apply a function and our output is y, and we apply another function and we have as an output z.\n",
        "#We want to obtain the minimum possible value of z.\n",
        "# In order to obtain dz/dx=dz/dy*dy/dx\n",
        "# 3 steps: 1) Forward pass: compute the loss.\n",
        "#          2) Compute local gradients.\n",
        "#          3) Backward pass: Compute dLoss/dWeihts using the chain rule.\n",
        "# yap= w*x         loss=(yap-y)^2= (wx-y)^2\n",
        "\n",
        "import torch\n",
        "\n",
        "x= torch.tensor(1.0)\n",
        "y= torch.tensor(2.0)\n",
        "\n",
        "w= torch.tensor(1.0, requires_grad=True) #We are interested in the gradient so we have to specify requires_grad=True\n",
        "\n",
        "#Forward pass and compute the loss\n",
        "y_hat=x*w\n",
        "loss= (y_hat-y)**2\n",
        "print(loss)\n",
        "\n",
        "#Backward pass\n",
        "\n",
        "loss.backward()\n",
        "print(w.grad)\n",
        "\n",
        "### update our weights \n",
        "### next forward and backwards\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# f= w*x\n",
        "\n",
        "# f= 2*x\n",
        "X= np.array([1,2,3,4],dtype=np.float32)\n",
        "Y= np.array([2,4,6,8], dtype=np.float32)\n",
        "\n",
        "w=0.0\n",
        "\n",
        "#Calculate our model prediction\n",
        "def forward(x):\n",
        "  return w*x\n",
        "\n",
        "#Calculate the loss, calculating it as the MSE\n",
        "def loss(y,y_predicted):\n",
        "  return ((y_predicted-y)**2).mean()\n",
        "\n",
        "#Calculate the gradient\n",
        "#MSE =1/N*(w*x-y)**2\n",
        "#dJ/dw=1/N*2x(wx-y)\n",
        "def gradient(x,y,y_predicted):\n",
        "  return np.dot(2*x,y_predicted-y).mean()\n",
        "\n",
        "print(f'Prediction before trainin: f(5)={forward(5):.3f}')\n",
        "\n",
        "#Training\n",
        "learning_rate=0.01\n",
        "n_iters=20\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  #prediction= forward pass\n",
        "  y_pred=forward(X)\n",
        "\n",
        "  #loss \n",
        "  l= loss(Y,y_pred)\n",
        "\n",
        "  #gradients\n",
        "  dw=gradient(X,Y,y_pred)\n",
        "\n",
        "  #update weights\n",
        "  w -= learning_rate*dw\n",
        "\n",
        "  if epoch % 2==0:\n",
        "    print(f'epoch {epoch+1}: w= {w:.3f}, loss= {l:.8f}')\n",
        "  \n",
        "  print(f'Prediction after the training: f(5)= {forward(5):.3f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cx63c-IqN_bY",
        "outputId": "6138fad3-0833-4f09-a68e-855ee953a612"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before trainin: f(5)=0.000\n",
            "epoch 1: w= 1.200, loss= 30.00000000\n",
            "Prediction after the training: f(5)= 6.000\n",
            "Prediction after the training: f(5)= 8.400\n",
            "epoch 3: w= 1.872, loss= 0.76800019\n",
            "Prediction after the training: f(5)= 9.360\n",
            "Prediction after the training: f(5)= 9.744\n",
            "epoch 5: w= 1.980, loss= 0.01966083\n",
            "Prediction after the training: f(5)= 9.898\n",
            "Prediction after the training: f(5)= 9.959\n",
            "epoch 7: w= 1.997, loss= 0.00050331\n",
            "Prediction after the training: f(5)= 9.984\n",
            "Prediction after the training: f(5)= 9.993\n",
            "epoch 9: w= 1.999, loss= 0.00001288\n",
            "Prediction after the training: f(5)= 9.997\n",
            "Prediction after the training: f(5)= 9.999\n",
            "epoch 11: w= 2.000, loss= 0.00000033\n",
            "Prediction after the training: f(5)= 10.000\n",
            "Prediction after the training: f(5)= 10.000\n",
            "epoch 13: w= 2.000, loss= 0.00000001\n",
            "Prediction after the training: f(5)= 10.000\n",
            "Prediction after the training: f(5)= 10.000\n",
            "epoch 15: w= 2.000, loss= 0.00000000\n",
            "Prediction after the training: f(5)= 10.000\n",
            "Prediction after the training: f(5)= 10.000\n",
            "epoch 17: w= 2.000, loss= 0.00000000\n",
            "Prediction after the training: f(5)= 10.000\n",
            "Prediction after the training: f(5)= 10.000\n",
            "epoch 19: w= 2.000, loss= 0.00000000\n",
            "Prediction after the training: f(5)= 10.000\n",
            "Prediction after the training: f(5)= 10.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "yyLZmwngTHD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "\n",
        "# f= w*x\n",
        "\n",
        "# f= 2*x\n",
        "X= torch.tensor([1,2,3,4],dtype=torch.float32)\n",
        "Y= torch.tensor([2,4,6,8], dtype=torch.float32)\n",
        "\n",
        "w=torch.tensor(0.0,dtype=torch.float32, requires_grad= True )\n",
        "\n",
        "#Calculate our model prediction\n",
        "def forward(x):\n",
        "  return w*x\n",
        "\n",
        "#Calculate the loss, calculating it as the MSE\n",
        "def loss(y,y_predicted):\n",
        "  return ((y_predicted-y)**2).mean()\n",
        "\n",
        "\n",
        "\n",
        "print(f'Prediction before trainin: f(5)={forward(5):.3f}')\n",
        "\n",
        "#Training\n",
        "learning_rate=0.01\n",
        "n_iters=100\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  #prediction= forward pass\n",
        "  y_pred=forward(X)\n",
        "\n",
        "  #loss \n",
        "  l= loss(Y,y_pred)\n",
        "\n",
        "  #gradients = backward pass\n",
        "  l.backward() #dl/dw\n",
        "\n",
        "  #update weights\n",
        "  with torch.no_grad():\n",
        "    w -= learning_rate*w.grad\n",
        "  #zero gradients \n",
        "  w.grad.zero_()\n",
        "\n",
        "\n",
        "  if epoch % 10==0:\n",
        "    print(f'epoch {epoch+1}: w= {w:.3f}, loss= {l:.8f}')\n",
        "  \n",
        "print(f'Prediction after the training: f(5)= {forward(5):.3f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4c68985-db96-432e-e7f5-b3240b2b928f",
        "id": "EheasI2uTYnE"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before trainin: f(5)=0.000\n",
            "epoch 1: w= 0.300, loss= 30.00000000\n",
            "epoch 11: w= 1.665, loss= 1.16278565\n",
            "epoch 21: w= 1.934, loss= 0.04506890\n",
            "epoch 31: w= 1.987, loss= 0.00174685\n",
            "epoch 41: w= 1.997, loss= 0.00006770\n",
            "epoch 51: w= 1.999, loss= 0.00000262\n",
            "epoch 61: w= 2.000, loss= 0.00000010\n",
            "epoch 71: w= 2.000, loss= 0.00000000\n",
            "epoch 81: w= 2.000, loss= 0.00000000\n",
            "epoch 91: w= 2.000, loss= 0.00000000\n",
            "Prediction after the training: f(5)= 10.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "cCaDa2IgavfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1) Design our model (input, output size, forward pass)\n",
        "#2) Construct loss and optimizer.\n",
        "#3) Training loop.\n",
        "#    -Forward pass: Compute prediction.\n",
        "#    -Backward pass: gradients.\n",
        "#    -Update weights.\n",
        "\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "\n",
        "# f= w*x\n",
        "\n",
        "# f= 2*x\n",
        "X= torch.tensor([1,2,3,4],dtype=torch.float32)\n",
        "Y= torch.tensor([2,4,6,8], dtype=torch.float32)\n",
        "\n",
        "w=torch.tensor(0.0,dtype=torch.float32, requires_grad= True )\n",
        "\n",
        "#Calculate our model prediction\n",
        "def forward(x):\n",
        "  return w*x\n",
        "\n",
        "print(f'Prediction before trainin: f(5)={forward(5):.3f}')\n",
        "\n",
        "#Training\n",
        "learning_rate=0.01\n",
        "n_iters=100\n",
        "\n",
        "loss = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD([w],lr=learning_rate)\n",
        "#SGD stands for Stochastic gradient descent\n",
        "for epoch in range(n_iters):\n",
        "  #prediction= forward pass\n",
        "  y_pred=forward(X)\n",
        "\n",
        "  #loss \n",
        "  l= loss(Y,y_pred)\n",
        "\n",
        "  #gradients = backward pass\n",
        "  l.backward() #dl/dw\n",
        "\n",
        "  #We don´t need to manually update our weights\n",
        "  optimizer.step()\n",
        "  #W still have to empty our gradients after the optimization step, zero gradients \n",
        "  optimizer.zero_grad()\n",
        "\n",
        "\n",
        "  if epoch % 10==0:\n",
        "    print(f'epoch {epoch+1}: w= {w:.3f}, loss= {l:.8f}')\n",
        "  \n",
        "print(f'Prediction after the training: f(5)= {forward(5):.3f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1f8cb1c-d737-4c58-d87f-0dc81169b6aa",
        "id": "P81JsYV4axPG"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before trainin: f(5)=0.000\n",
            "epoch 1: w= 0.300, loss= 30.00000000\n",
            "epoch 11: w= 1.665, loss= 1.16278565\n",
            "epoch 21: w= 1.934, loss= 0.04506890\n",
            "epoch 31: w= 1.987, loss= 0.00174685\n",
            "epoch 41: w= 1.997, loss= 0.00006770\n",
            "epoch 51: w= 1.999, loss= 0.00000262\n",
            "epoch 61: w= 2.000, loss= 0.00000010\n",
            "epoch 71: w= 2.000, loss= 0.00000000\n",
            "epoch 81: w= 2.000, loss= 0.00000000\n",
            "epoch 91: w= 2.000, loss= 0.00000000\n",
            "Prediction after the training: f(5)= 10.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#STEP 4\n"
      ],
      "metadata": {
        "id": "1MDYN05EcLHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "\n",
        "# f= w*x\n",
        "\n",
        "# f= 2*x\n",
        "X= torch.tensor([[1],[2],[3],[4]],dtype=torch.float32)\n",
        "Y= torch.tensor([[2],[4],[6],[8]], dtype=torch.float32)\n",
        "\n",
        "X_test= torch.tensor([5], dtype= torch.float32)\n",
        "\n",
        "n_samples, n_features= X.shape\n",
        "print( n_samples, n_features)\n",
        "\n",
        "input_size= n_features\n",
        "output_size= n_features\n",
        "\n",
        "model=nn.Linear(input_size, output_size)\n",
        "#This is only one layer, it needs an input size and output size of our features.\n",
        "#Now or X and Y need both a different shape, it has to be a 2D array with the number of rows as the number of samples and for each row we have the number of features.\n",
        "\n",
        "print(f'Prediction before trainin: f(5)={model(X_test).item():.3f}')\n",
        "\n",
        "#Training\n",
        "learning_rate=0.01\n",
        "n_iters=100\n",
        "\n",
        "loss = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
        "#SGD stands for Stochastic gradient descent\n",
        "for epoch in range(n_iters):\n",
        "  #prediction= forward pass\n",
        "  y_pred=model(X)\n",
        "\n",
        "  #loss \n",
        "  l= loss(Y,y_pred)\n",
        "\n",
        "  #gradients = backward pass\n",
        "  l.backward() #dl/dw\n",
        "\n",
        "  #We don´t need to manually update our weights\n",
        "  optimizer.step()\n",
        "  #W still have to empty our gradients after the optimization step, zero gradients \n",
        "  optimizer.zero_grad()\n",
        "\n",
        "\n",
        "  if epoch % 10==0:\n",
        "    [w,b]= model.parameters()\n",
        "    print(f'epoch {epoch+1}: w= {w[0][0].item():.3f}, loss= {l:.8f}')\n",
        "  \n",
        "print(f'Prediction after the training: f(5)= {model(X_test).item():.3f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f7f567e-fe42-4b9a-a14a-7572ca770938",
        "id": "SsWeeDfacUdU"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 1\n",
            "Prediction before trainin: f(5)=4.316\n",
            "epoch 1: w= 1.078, loss= 10.16656590\n",
            "epoch 11: w= 1.810, loss= 0.26658076\n",
            "epoch 21: w= 1.929, loss= 0.01023797\n",
            "epoch 31: w= 1.949, loss= 0.00341124\n",
            "epoch 41: w= 1.954, loss= 0.00305149\n",
            "epoch 51: w= 1.955, loss= 0.00286971\n",
            "epoch 61: w= 1.957, loss= 0.00270258\n",
            "epoch 71: w= 1.958, loss= 0.00254526\n",
            "epoch 81: w= 1.959, loss= 0.00239711\n",
            "epoch 91: w= 1.961, loss= 0.00225759\n",
            "Prediction after the training: f(5)= 9.921\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GB46wUFWfZP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "\n",
        "# f= w*x\n",
        "\n",
        "# f= 2*x\n",
        "X= torch.tensor([[1],[2],[3],[4]],dtype=torch.float32)\n",
        "Y= torch.tensor([[2],[4],[6],[8]], dtype=torch.float32)\n",
        "\n",
        "X_test= torch.tensor([5], dtype= torch.float32)\n",
        "\n",
        "n_samples, n_features= X.shape\n",
        "print( n_samples, n_features)\n",
        "\n",
        "input_size= n_features\n",
        "output_size= n_features\n",
        "\n",
        "\n",
        "class LinearRegression(nn.Module):\n",
        "  def __init__(self, input_dim, output_dim):\n",
        "    super(LinearRegression,self).__init__()\n",
        "    #define our layers\n",
        "    self.lin= nn.Linear(input_dim,output_dim)\n",
        "  \n",
        "  def forward(self,x):\n",
        "    return self.lin(x)\n",
        "\n",
        "model=LinearRegression(input_size, output_size)\n",
        "\n",
        "\n",
        "#Now or X and Y need both a different shape, it has to be a 2D array with the number of rows as the number of samples and for each row we have the number of features.\n",
        "\n",
        "print(f'Prediction before trainin: f(5)={model(X_test).item():.3f}')\n",
        "\n",
        "#Training\n",
        "learning_rate=0.01\n",
        "n_iters=100\n",
        "\n",
        "loss = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
        "#SGD stands for Stochastic gradient descent\n",
        "for epoch in range(n_iters):\n",
        "  #prediction= forward pass\n",
        "  y_pred=model(X)\n",
        "\n",
        "  #loss \n",
        "  l= loss(Y,y_pred)\n",
        "\n",
        "  #gradients = backward pass\n",
        "  l.backward() #dl/dw\n",
        "\n",
        "  #We don´t need to manually update our weights\n",
        "  optimizer.step()\n",
        "  #W still have to empty our gradients after the optimization step, zero gradients \n",
        "  optimizer.zero_grad()\n",
        "\n",
        "\n",
        "  if epoch % 10==0:\n",
        "    [w,b]= model.parameters()\n",
        "    print(f'epoch {epoch+1}: w= {w[0][0].item():.3f}, loss= {l:.8f}')\n",
        "  \n",
        "print(f'Prediction after the training: f(5)= {model(X_test).item():.3f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58accb1f-9fc7-40d7-b7ef-ee4cfd8640db",
        "id": "Dpzqu58BfauQ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 1\n",
            "Prediction before trainin: f(5)=-2.279\n",
            "epoch 1: w= 0.031, loss= 47.96054840\n",
            "epoch 11: w= 1.619, loss= 1.24921679\n",
            "epoch 21: w= 1.876, loss= 0.04019616\n",
            "epoch 31: w= 1.920, loss= 0.00845763\n",
            "epoch 41: w= 1.928, loss= 0.00720477\n",
            "epoch 51: w= 1.932, loss= 0.00676574\n",
            "epoch 61: w= 1.934, loss= 0.00637142\n",
            "epoch 71: w= 1.936, loss= 0.00600055\n",
            "epoch 81: w= 1.938, loss= 0.00565128\n",
            "epoch 91: w= 1.939, loss= 0.00532235\n",
            "Prediction after the training: f(5)= 9.879\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#LINEAR REGRESSION \n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn import datasets \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 0) prepare our data\n",
        "X_numpy, y_numpy= datasets.make_regression(n_samples=100, n_features=1, noise=20, random_state=1)\n",
        "X= torch.from_numpy(X_numpy.astype(np.float32))\n",
        "y= torch.from_numpy(y_numpy.astype(np.float32))\n",
        "y=y.view(y.shape[0],1)\n",
        "n_samples, n_features = X.shape\n",
        "\n",
        "\n",
        "# 1) model\n",
        "input_size= n_features\n",
        "output_size= 1\n",
        "model= nn.Linear(input_size, output_size)\n",
        "\n",
        "# 2) define the loss and optimizer \n",
        "learning_rate= 0.01\n",
        "criterion= nn.MSELoss()\n",
        "optimizer= torch.optim.SGD(model.parameters(),lr= learning_rate)\n",
        "\n",
        "# 3) training loop\n",
        "num_epochs= 100\n",
        "for epoch in range(num_epochs):\n",
        "   #forward pass and loss\n",
        "   y_predicted= model(X)\n",
        "   loss=criterion(y_predicted,y)\n",
        "\n",
        "   #backward pass\n",
        "   loss.backward()\n",
        "\n",
        "   #update\n",
        "   optimizer.step()\n",
        "\n",
        "   #empty our gradients\n",
        "   optimizer.zero_grad()\n",
        "\n",
        "   if (epoch+1) %10==0:\n",
        "     print(f'epoch: {epoch+1}, loss ={loss.item():.4f}')\n",
        "\n",
        "#plot\n",
        "predicted= model(X).detach().numpy()\n",
        "plt.plot(X_numpy, Y_numpy, 'ro')\n",
        "plt.plot(X_numpy, predicted, 'b')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mFVTCYhGgxfI",
        "outputId": "1864a6e7-7650-4f14-c9fa-2af547ce398a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 10, loss =4287.8638\n",
            "epoch: 20, loss =3201.4875\n",
            "epoch: 30, loss =2415.3369\n",
            "epoch: 40, loss =1845.8413\n",
            "epoch: 50, loss =1432.8875\n",
            "epoch: 60, loss =1133.1753\n",
            "epoch: 70, loss =915.4694\n",
            "epoch: 80, loss =757.2095\n",
            "epoch: 90, loss =642.0818\n",
            "epoch: 100, loss =558.2772\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#LOGISTIC REGESSION\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#0) Prepare data\n",
        "\n",
        "bc=datasets.load_breast_cancer()\n",
        "# Binary classification problem where we can predict cancer based on the input features.\n",
        "X,y= bc.data, bc.target\n",
        "\n",
        "n_samples, n_features= X.shape\n",
        "print(n_samples, n_features)\n",
        "X_train, X_test, y_train, y_test= train_test_split(X,y, test_size=0.2, random_state=1234)\n",
        "\n",
        "#scale our features\n",
        "sc= StandardScaler()\n",
        "X_train=sc.fit_transform(X_train)\n",
        "X_test= sc.transform(X_test)\n",
        "\n",
        "X_train= torch.from_numpy(X_train.astype(np.float32))\n",
        "X_test= torch.from_numpy(X_test.astype(np.float32))\n",
        "y_train= torch.from_numpy(y_train.astype(np.float32))\n",
        "y_test= torch.from_numpy(y_test.astype(np.float32))\n",
        "\n",
        "y_train=y_train.view(y_train.shape[0],1)\n",
        "y_test= y_test.view(y_test.shape[0],1)\n",
        "\n",
        "#1) Model\n",
        "# f=wx+b, sigmoid at the end\n",
        "class LogisticRegression(nn.Module):\n",
        "\n",
        "  def __init__(self, n_input_features):\n",
        "    super(LogisticRegression, self).__init__()\n",
        "    self.linear= nn.Linear(n_input_features,1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    y_predicted= torch.sigmoid(self.linear(x))\n",
        "    return y_predicted\n",
        "\n",
        "model=LogisticRegression(n_features)\n",
        "\n",
        "#2) loss and optimizer\n",
        "learning_rate=0.01\n",
        "criterion= nn.BCELoss()\n",
        "optimizer= torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
        "\n",
        "#3) training loop\n",
        "num_epochs= 100\n",
        "for epoch in range(num_epochs):\n",
        "  #forward pass and loss\n",
        "  y_predicted= model(X_train)\n",
        "  loss=criterion(y_predicted, y_train)\n",
        "\n",
        "  #backward pass\n",
        "  loss.backward()\n",
        "\n",
        "  #updates\n",
        "  optimizer.step()\n",
        "\n",
        "  #zero gradients\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if (epoch+1)%10==0:\n",
        "    print(f'epoch:{epoch+1},loss={loss.item():.4f}')\n",
        "\n",
        "with torch.no_grad():\n",
        "  y_predicted=model(X_test)\n",
        "  y_predicted_cls= y_predicted.round()\n",
        "  acc= y_predicted_cls.eq(y_test).sum()/float(y_test.shape[0])\n",
        "  print(f'accuracy: {acc:.4f}')\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "s4na0p5UqfaT",
        "outputId": "3750577b-6b23-4fa2-98e8-4056e7aefd53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "569 30\n",
            "epoch:10,loss=0.6167\n",
            "epoch:20,loss=0.4969\n",
            "epoch:30,loss=0.4238\n",
            "epoch:40,loss=0.3746\n",
            "epoch:50,loss=0.3390\n",
            "epoch:60,loss=0.3118\n",
            "epoch:70,loss=0.2902\n",
            "epoch:80,loss=0.2726\n",
            "epoch:90,loss=0.2579\n",
            "epoch:100,loss=0.2454\n",
            "accuracy: 0.9035\n"
          ]
        }
      ]
    }
  ]
}